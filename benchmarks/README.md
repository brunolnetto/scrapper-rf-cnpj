# Enhanced CNPJ Benchmark Suite

## Overview

This enhanced benchmark suite implements comprehensive performance testing for Brazilian Federal Revenue CNPJ data processing using modern data engineering practices. The suite provides:

- **Multi-Engine Support**: Polars and DuckDB benchmarks with optimized configurations
- **Memory Safety**: Real-time monitoring with automatic alerts and emergency stops
- **Parallel Execution**: Resource-aware parallel processing with memory constraints
- **Comprehensive Analysis**: Trend analysis, run comparisons, and detailed reporting
- **Error Recovery**: Robust error handling with retry mechanisms and fallback strategies

## Architecture

### Core Components

```
benchmarks/
├── config.py              # Centralized configuration system
├── utils.py               # Enhanced utilities with memory safety
├── base.py                # Abstract base classes
├── monitoring.py          # Real-time monitoring system
├── analysis.py            # Enhanced reporting and analysis
├── parallel.py            # Parallel execution with resource management
├── orchestrator.py        # Main orchestration system
├── polars_enhanced.py     # Enhanced Polars benchmarks
├── duckdb_enhanced.py     # Enhanced DuckDB benchmarks
└── enhanced_runner.py     # Command-line interface
```

### Key Features

#### 1. Memory Safety
- **Real-time monitoring** with configurable thresholds
- **Memory estimation** before benchmark execution
- **Emergency stops** on critical memory pressure
- **Garbage collection** optimization

#### 2. Error Handling
- **Status-based tracking** (COMPLETED, FAILED, SKIPPED, RUNNING)
- **Recovery strategies** with retry mechanisms
- **Graceful degradation** for failed benchmarks
- **Comprehensive error reporting**

#### 3. Configuration Management
- **Centralized configuration** with BenchmarkConfig
- **Brazilian CNPJ defaults** (ISO-8859-1 encoding, ';' delimiter)
- **Auto-detection** of system resources
- **Environment-specific** overrides

#### 4. Parallel Processing
- **Resource-aware scheduling** based on memory requirements
- **Group-based execution** to prevent memory exhaustion
- **Concurrent monitoring** with real-time alerts
- **Automatic load balancing**

## Quick Start

### Installation

```bash
# Install dependencies
pip install polars duckdb psutil

# Verify installation
python -c "import polars, duckdb, psutil; print('All dependencies available')"
```

### Basic Usage

```bash
# Simple benchmark on CSV files
python benchmarks/enhanced_runner.py --files data/*.csv --engines polars duckdb

# Custom memory limit and parallel execution
python benchmarks/enhanced_runner.py --files data/*.csv --memory-limit 16 --parallel

# Ingestion benchmarks only
python benchmarks/enhanced_runner.py --files data/*.csv --ingestion-only

# Query benchmarks with specific data file
python benchmarks/enhanced_runner.py --files data/empresas.csv --queries-only
```

## Legacy Benchmarks (Still Available)

The original benchmark suite is still available for compatibility:

```bash
# Quick test with original benchmarks
python3 -m benchmarks.quick_benchmark

# Full benchmark on estabelecimento files  
python3 -m benchmarks.run_benchmarks --data-dir data --pattern "*ESTABELE*"

# Analyze original results
python3 -m benchmarks.analyze_results benchmark_output/benchmark_results_*.json
```

### Test Scenarios

#### DuckDB Tests
1. **CSV Ingestion**: `CREATE TABLE AS SELECT * FROM read_csv_auto(...)`
2. **SQL Aggregation**: GROUP BY and COUNT operations
3. **Parquet Export**: COPY TO parquet with compression

#### Polars Tests
1. **Streaming Ingestion**: `scan_csv` with `collect(streaming=True)`
2. **Eager Ingestion**: Standard `read_csv` approach
3. **Chunked Processing**: Batch processing for memory-constrained scenarios
4. **Parquet Aggregation**: Lazy evaluation on parquet files

## Output Files

After running benchmarks, you'll get:

```
benchmark_output/
├── benchmark_results_YYYYMMDD_HHMMSS.json    # Raw results data
├── benchmark_report_YYYYMMDD_HHMMSS.md       # Human-readable report
├── duckdb/
│   └── duckdb_output.parquet                 # DuckDB output
├── polars/
│   ├── polars_output_streaming.parquet       # Polars streaming output
│   ├── polars_output_eager.parquet           # Polars eager output
│   └── polars_chunked_output.parquet         # Polars chunked output
└── analysis_output/                          # Generated by analyze_results.py
    ├── benchmark_visualization.png
    ├── performance_scatter.png
    ├── comparison_table.csv
    └── comparison_table.html
```

## Example Results Interpretation

### Speed Winner
```
🏆 DuckDB is 2.3x faster than Polars (streaming)
Duration: DuckDB 45.2s vs Polars 103.7s
```

### Memory Winner
```
🧠 Polars uses 1.8x less memory than DuckDB
Peak Memory: Polars 3.2GB vs DuckDB 5.8GB
```

### Compression Winner
```
📦 Both achieve ~3.5x compression (25GB → 7GB)
```

## Configuration Options

### Memory Limits
```bash
# Constrain DuckDB memory usage
python benchmarks/run_benchmarks.py --memory-limit 4GB

# Reduce Polars thread count (in code)
PolarsBenchmark(max_threads=4)
```

### File Selection
```bash
# Test specific file patterns
python benchmarks/run_benchmarks.py --pattern "*EMPRESA*"    # Company files
python benchmarks/run_benchmarks.py --pattern "*SOCIO*"     # Partner files
python benchmarks/run_benchmarks.py --pattern "*ESTABELE*"  # Establishment files

# Limit number of files for testing
python benchmarks/run_benchmarks.py --max-files 3
```

## Understanding Your Results

### When DuckDB Wins
- **Simple ingestion workflows**: SQL `CREATE TABLE AS SELECT` is extremely robust
- **Memory-constrained environments**: Automatic spilling to disk
- **Complex SQL operations**: Window functions, complex joins
- **Integration with PostgreSQL**: Direct COPY operations

### When Polars Wins  
- **Complex data transformations**: Rich DataFrame API
- **Streaming pipelines**: Lazy evaluation with memory control
- **Python ecosystem integration**: Native DataFrame objects
- **Custom processing logic**: UDFs and complex operations

### When to Use Hybrid Approach
Best of both worlds:
1. **DuckDB for ingestion**: Robust CSV → Parquet conversion
2. **Polars for transformations**: Complex data cleaning and processing
3. **PostgreSQL for persistence**: Final data storage

## Troubleshooting

### No Files Found
```bash
# Check your data directory structure
ls data/EXTRACTED_FILES/*ESTABELE*

# Verify file pattern
python benchmarks/run_benchmarks.py --data-dir data/EXTRACTED_FILES --pattern "*"
```

### Memory Issues
```bash
# Reduce memory limit
python benchmarks/run_benchmarks.py --memory-limit 2GB --max-files 1

# Run only one tool at a time
python benchmarks/run_benchmarks.py --skip-polars  # DuckDB only
python benchmarks/run_benchmarks.py --skip-duckdb  # Polars only
```

### Import Errors
```bash
# Install required packages
pip install duckdb polars pandas matplotlib seaborn psutil
```

## Customization

### Add Custom Benchmarks
Extend the benchmark classes in:
- `benchmarks/duckdb_benchmark.py` - Add new DuckDB tests
- `benchmarks/polars_benchmark.py` - Add new Polars tests

### Modify Test Data
Edit `find_cnpj_files()` in `run_benchmarks.py` to change file discovery logic.

### Custom Metrics
Add new measurements to `BenchmarkResult` class in `benchmarks/utils.py`.

---

## Expected Performance Patterns

Based on your 25GB→7GB compression success, expect:

- **DuckDB**: More consistent memory usage, slower on complex transformations
- **Polars**: Higher peak memory, faster on vectorized operations
- **Both**: Similar compression ratios (~3.5x for CNPJ data)

The benchmarks will help you choose the right tool for your specific use case! 🚀