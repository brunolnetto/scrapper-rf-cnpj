# Pipeline ETL - Dados PÃºblicos CNPJ

## ğŸ“Š VisÃ£o Geral

Este projeto implementa um pipeline ETL robusto e escalÃ¡vel para processamento de dados pÃºblicos do CNPJ da Receita Federal do Brasil. O sistema Ã© capaz de processar grandes volumes de dados (~17GB descompactados) com auditoria completa e carregamento incremental.

### ğŸ—ï¸ Modelo de Entidade Relacionamento

![Modelo ERD](https://github.com/brunolnetto/RF_CNPJ/blob/master/images/Dados_RFB_ERD.png)

### âœ¨ CaracterÃ­sticas Principais

- **ETL de Alta Performance**: Sistema async com processamento paralelo interno
- **DetecÃ§Ã£o Robusta de Arquivos**: Sistema de 4 camadas com validaÃ§Ã£o de conteÃºdo
- **Processamento Incremental**: Suporte a carregamento por ano/mÃªs especÃ­fico
- **Auditoria Completa**: Rastreamento de todas as operaÃ§Ãµes com checksums e metadados
- **Formato Otimizado**: ConversÃ£o automÃ¡tica CSV â†’ Parquet para melhor performance
- **Arquitetura Dual**: Bancos separados para produÃ§Ã£o e auditoria
- **Sistema de Logs**: Logging estruturado em JSON com rotaÃ§Ã£o automÃ¡tica
- **Streaming de Dados**: Processamento de arquivos grandes sem carregar na memÃ³ria
- **TolerÃ¢ncia a Falhas**: Retry automÃ¡tico com backoff exponencial

### ğŸ”„ Fluxo do Pipeline

1. **Download** - Baixa arquivos ZIP da fonte oficial com retry inteligente
2. **ExtraÃ§Ã£o** - Descompacta arquivos CSV com validaÃ§Ã£o de integridade
3. **ConversÃ£o** - Transforma dados para formato Parquet otimizado (polars)
4. **Carregamento** - Sistema async de alta performance com upserts em batch

### ğŸš€ Arquitetura

- **Carregador de Arquivos Aprimorado**: DetecÃ§Ã£o automÃ¡tica de formato (CSV/Parquet) com validaÃ§Ã£o robusta
- **Processamento AssÃ­ncrono**: Processamento paralelo interno com controle de concorrÃªncia
- **EstratÃ©gia de Carregamento Unificado**: Interface simplificada para todos os tipos de arquivo
- **Pool de ConexÃµes**: Pool de conexÃµes async para mÃ¡xima performance
- **Eficiente em MemÃ³ria**: Streaming processing para arquivos de qualquer tamanho

## ğŸ“š Fonte de Dados

- **Dados Oficiais**: [Portal de Dados Abertos do Governo](https://dados.gov.br/dados/conjuntos-dados/cadastro-nacional-da-pessoa-juridica---cnpj)
- **Layout dos Dados**: [Metadados CNPJ - Receita Federal](https://www.gov.br/receitafederal/dados/cnpj-metadados.pdf)
- **Volume**: ~4.7GB compactado | ~17.1GB descompactado
- **AtualizaÃ§Ã£o**: Mensal pela Receita Federal

## ğŸ› ï¸ Requisitos do Sistema

### Infraestrutura
- **Python**: 3.9+ (recomendado 3.11+)
- **PostgreSQL**: 14.2+ 
- **MemÃ³ria RAM**: MÃ­nimo 8GB (recomendado 16GB+)
- **EspaÃ§o em Disco**: ~50GB livres para processamento

### DependÃªncias Python
- **Gerenciador**: [uv](https://github.com/astral-sh/uv) (recomendado) ou pip
- **Principais**: asyncpg (async database), pyarrow (file processing), sqlalchemy (ORM)
- **Performance**: polars (conversÃ£o CSVâ†’Parquet apenas)
- **UtilitÃ¡rios**: rich (progress), pydantic (validation), pathlib (paths)
- **Ver**: `requirements.txt` para lista completa

### ğŸ“ˆ Performance
- **Uso de MemÃ³ria**: ~70% de reduÃ§Ã£o via streaming processing
- **Velocidade de Processamento**: 2-3x mais rÃ¡pido com paralelismo assÃ­ncrono interno  
- **DetecÃ§Ã£o de Arquivos**: Falhas prÃ³ximas de zero com validaÃ§Ã£o de 4 camadas
- **Escalabilidade**: Processa 60M+ registros eficientemente

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. Setup do Banco de Dados
```bash
# Conecte ao PostgreSQL
sudo -u postgres psql

# Configure usuÃ¡rio e senha
ALTER USER postgres PASSWORD 'sua_senha_aqui';

# Crie os bancos (produÃ§Ã£o e auditoria)
CREATE DATABASE dadosrfb;
CREATE DATABASE dadosrfb_analysis;
```

### 2. ConfiguraÃ§Ã£o do Ambiente
```bash
# Clone o repositÃ³rio
git clone https://github.com/brunolnetto/scrapper-rf-cnpj.git
cd scrapper-rf-cnpj

# Copie e configure o arquivo de ambiente
cp .env.template .env
# Edite o .env com suas credenciais
```

### 3. InstalaÃ§Ã£o das DependÃªncias
```bash
# OpÃ§Ã£o 1: Usando uv (recomendado)
pip install uv
uv pip install -r requirements.txt

# OpÃ§Ã£o 2: Usando pip tradicional
pip install -r requirements.txt
```

### 4. ConfiguraÃ§Ã£o do `.env`

> ğŸ“– **DocumentaÃ§Ã£o Completa**: Consulte [docs/ENVIRONMENT_VARIABLES.md](docs/ENVIRONMENT_VARIABLES.md) para detalhes completos sobre todas as variÃ¡veis de ambiente.

```env
# Ambiente
ENVIRONMENT=development

# Banco Principal
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=sua_senha
POSTGRES_DBNAME=dadosrfb

# Banco de Auditoria
AUDIT_DB_HOST=localhost
AUDIT_DB_PORT=5432
AUDIT_DB_USER=postgres
AUDIT_DB_PASSWORD=sua_senha
AUDIT_DB_NAME=dadosrfb_analysis

# DiretÃ³rios (opcionais)
OUTPUT_PATH=data/DOWNLOAD_FILES
EXTRACT_PATH=data/EXTRACTED_FILES

# Carregamento
ETL_CHUNK_SIZE=50000
ETL_SUB_BATCH_SIZE=5000
ETL_INTERNAL_CONCURRENCY=3
ETL_ASYNC_POOL_MIN_SIZE=2
ETL_ASYNC_POOL_MAX_SIZE=10
```

### ğŸ”§ ValidaÃ§Ã£o da ConfiguraÃ§Ã£o

Para validar sua configuraÃ§Ã£o antes de executar o ETL:

```bash
# Validar configuraÃ§Ã£o do ambiente
python scripts/validate_env.py

# Guia rÃ¡pido de referÃªncia
cat docs/ENV_QUICK_REFERENCE.md
```

### ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

O sistema suporta configuraÃ§Ãµes avanÃ§adas para otimizaÃ§Ã£o de performance:

- **`ETL_CHUNK_SIZE`**: Tamanho do batch principal (padrÃ£o: 50,000)
- **`ETL_SUB_BATCH_SIZE`**: Tamanho dos sub-batches internos (padrÃ£o: 5,000)  
- **`ETL_INTERNAL_CONCURRENCY`**: Paralelismo interno por arquivo (padrÃ£o: 3)
- **`ETL_ASYNC_POOL_*`**: ConfiguraÃ§Ãµes do pool de conexÃµes async

> ğŸ’¡ **Dica**: Use `python scripts/validate_env.py` para recomendaÃ§Ãµes especÃ­ficas baseadas nos recursos do seu sistema.

### ğŸ“ Arquivos Suportados

O sistema detecta automaticamente o formato dos arquivos:

- **CSV**: `.csv`, `.txt`, `.dat` - com detecÃ§Ã£o automÃ¡tica de delimitador
- **Parquet**: `.parquet` - com validaÃ§Ã£o de magic bytes
- **Encoding**: DetecÃ§Ã£o automÃ¡tica com fallback para encoding especÃ­fico por tabela
## â–¶ï¸ Como Executar

### Comandos Principais (usando just)
```bash
# Executar ETL para mÃªs/ano atual
just run

# Executar para perÃ­odo especÃ­fico
just run-etl 2024 12

# Ver todos os comandos disponÃ­veis
just help
```

### ExecuÃ§Ã£o Manual
```bash
# ETL para perÃ­odo atual
python -m src.main

# ETL para perÃ­odo especÃ­fico
python -m src.main --year 2024 --month 12

# ETL com refresh completo (limpa tabelas)
python -m src.main --year 2024 --month 12 --full-refresh true

# Limpar tabelas especÃ­ficas
python -m src.main --clear-tables "empresa,estabelecimento"
```

### Comandos de Desenvolvimento
```bash
# Instalar dependÃªncias
just install

# Executar linting
just lint

# Limpar logs e cache
just clean

# Buscar no cÃ³digo
just search "termo_pesquisa"
```

### â±ï¸ Tempo de Processamento
- **Dados completos**: 4-8 horas (dependendo da infraestrutura)
- **Processamento incremental**: 30min - 2h
- **Download**: 1-2 horas (dependendo da conexÃ£o)
- **Logs**: DisponÃ­veis em `logs/YYYY-MM-DD/HH_MM/`

## ğŸ—ƒï¸ Estrutura das Tabelas

Para informaÃ§Ãµes detalhadas, consulte o [layout oficial](https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cadastros/consultas/arquivos/NOVOLAYOUTDOSDADOSABERTOSDOCNPJ.pdf).

### Tabelas Principais (com Ã­ndices em `cnpj_basico`)
| Tabela | DescriÃ§Ã£o | Registros Aprox. |
|--------|-----------|------------------|
| `empresa` | Dados cadastrais da matriz | ~60M |
| `estabelecimento` | Dados por unidade/filial (endereÃ§os, telefones) | ~60M |
| `socios` | Dados dos sÃ³cios das empresas | ~30M |
| `simples` | Dados de MEI e Simples Nacional | ~40M |

### Tabelas de ReferÃªncia
| Tabela | DescriÃ§Ã£o |
|--------|-----------|
| `cnae` | CÃ³digos e descriÃ§Ãµes de atividades econÃ´micas |
| `quals` | QualificaÃ§Ãµes de pessoas fÃ­sicas (sÃ³cios, responsÃ¡veis) |
| `natju` | Naturezas jurÃ­dicas |
| `moti` | Motivos de situaÃ§Ã£o cadastral |
| `pais` | CÃ³digos de paÃ­ses |
| `munic` | CÃ³digos de municÃ­pios |

### ğŸ”— Relacionamentos
- **Chave Principal**: `cnpj_basico` (8 primeiros dÃ­gitos do CNPJ)
- **CNPJ Completo**: `cnpj_basico` + `cnpj_ordem` + `cnpj_dv` (em `estabelecimento`)
- **Ãndices**: Otimizados para consultas por CNPJ e relacionamentos

## ğŸ“ Estrutura do Projeto

```
scrapper-rf-cnpj/
â”œâ”€â”€ src/                   # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ main.py            # Ponto de entrada do ETL
â”‚   â”œâ”€â”€ core/              # Componentes principais do ETL
â”‚   â”œâ”€â”€ database/          # Modelos e conexÃµes de banco
â”‚   â”œâ”€â”€ setup/             # ConfiguraÃ§Ãµes e logging
â”‚   â””â”€â”€ utils/             # UtilitÃ¡rios diversos
â”œâ”€â”€ data/                  # Dados processados
â”‚   â”œâ”€â”€ DOWNLOAD_FILES/    # Arquivos ZIP baixados
â”‚   â”œâ”€â”€ EXTRACTED_FILES/   # Arquivos CSV extraÃ­dos
â”‚   â””â”€â”€ CONVERTED_FILES/   # Arquivos Parquet convertidos
â”œâ”€â”€ examples/              # Exemplos de uso
â”œâ”€â”€ lab/                   # Notebooks para anÃ¡lise
â”œâ”€â”€ logs/                  # Logs do sistema
â”œâ”€â”€ justfile               # Comandos automatizados
â”œâ”€â”€ requirements.txt       # DependÃªncias Python
â””â”€â”€ .env.template          # Template de configuraÃ§Ã£o
```

## ğŸ”§ Recursos AvanÃ§ados

### Sistema de Auditoria
- Rastreamento completo de operaÃ§Ãµes
- Metadados de arquivos processados
- Tempos de execuÃ§Ã£o e logs estruturados
- Banco separado para dados de auditoria

### OtimizaÃ§Ãµes de Performance
- ConversÃ£o automÃ¡tica para formato Parquet
- Carregamento em lotes (chunking)
- Ãndices otimizados no PostgreSQL
- Processamento paralelo quando possÃ­vel

### Monitoramento
- Logs em formato JSON estruturado
- Contadores de registros processados
- MÃ©tricas de tempo de execuÃ§Ã£o
- ValidaÃ§Ã£o de integridade dos dados

## ğŸ› ï¸ Desenvolvimento

### Estrutura de CÃ³digo
- **ConfiguraÃ§Ã£o Centralizada**: `src/setup/config.py`
- **PadrÃ£o Lazy Loading**: ConexÃµes de banco sob demanda
- **Strategy Pattern**: Diferentes estratÃ©gias de carregamento
- **Auditoria Integrada**: Rastreamento automÃ¡tico de operaÃ§Ãµes

### Executando Testes
```bash
# Usar exemplos para validaÃ§Ã£o
python examples/loading_example.py

# AnÃ¡lise exploratÃ³ria
jupyter lab lab/main.ipynb
```

### Contribuindo
1. Fork o projeto
2. Crie uma branch para sua feature
3. Execute os testes e linting
4. Submeta um Pull Request

## ğŸ“ Exemplos de Uso

### Consultas SQL BÃ¡sicas
```sql
-- Empresas ativas por estado
SELECT uf, COUNT(*) as total_empresas
FROM estabelecimento 
WHERE situacao_cadastral = '02'
GROUP BY uf
ORDER BY total_empresas DESC;

-- CNPJs completos com razÃ£o social
SELECT 
    CONCAT(e.cnpj_basico, est.cnpj_ordem, est.cnpj_dv) as cnpj_completo,
    e.razao_social,
    est.nome_fantasia
FROM empresa e
JOIN estabelecimento est ON e.cnpj_basico = est.cnpj_basico
WHERE est.identificador_matriz_filial = '1';
```

### AnÃ¡lise de Dados
```python
import pandas as pd
from sqlalchemy import create_engine

# Conectar ao banco
engine = create_engine('postgresql://user:pass@localhost/dadosrfb')

# AnÃ¡lise por setor
df = pd.read_sql("""
    SELECT c.descricao as setor, COUNT(*) as empresas
    FROM empresa e
    JOIN estabelecimento est ON e.cnpj_basico = est.cnpj_basico
    JOIN cnae c ON est.cnae_fiscal_principal = c.codigo
    GROUP BY c.descricao
    ORDER BY empresas DESC
    LIMIT 20
""", engine)
```

## ğŸ†˜ Troubleshooting

### Problemas Comuns
- **Erro de conexÃ£o**: Verifique credenciais no `.env`
- **Falta de espaÃ§o**: Monitore diretÃ³rio `data/`
- **Timeout de download**: Verifique conexÃ£o de internet
- **MemÃ³ria insuficiente**: Ajuste `chunk_size` na configuraÃ§Ã£o

### Logs e Debugging
- Logs detalhados em `logs/YYYY-MM-DD/HH_MM/`
- Use `just clean` para limpar caches
- Verifique status das tabelas no banco de auditoria

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ¤ Contribuidores

- [Bruno Peixoto](https://github.com/brunolnetto) - Autor principal

## ğŸ“§ Contato

Para dÃºvidas ou sugestÃµes, abra uma [issue](https://github.com/brunolnetto/scrapper-rf-cnpj/issues) no GitHub.

---

â­ **Se este projeto foi Ãºtil, considere dar uma estrela no repositÃ³rio!**
