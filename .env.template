# Possible values: development, production
# Controls overall system behavior and enables development-specific optimizations
ENVIRONMENT=development

################ Main database information
# PostgreSQL host for main CNPJ data storage
POSTGRES_HOST=localhost

# PostgreSQL port for main database
POSTGRES_PORT=5432

# PostgreSQL username for main database
POSTGRES_USER=postgres

# PostgreSQL password for main database
POSTGRES_PASSWORD=postgres

# Main database name for storing processed CNPJ data
POSTGRES_DBNAME=dadosrfb

# Maintenance database for administrative operations
POSTGRES_MAINTENANCE_DB=postgres

################ Audit database information
# PostgreSQL host for audit/analysis database
AUDIT_DB_HOST=localhost

# PostgreSQL port for audit database
AUDIT_DB_PORT=5432

# PostgreSQL username for audit database
AUDIT_DB_USER=postgres

# PostgreSQL password for audit database
AUDIT_DB_PASSWORD=postgres

# Audit database name for storing processing metadata and logs
AUDIT_DB_NAME=dadosrfb_analysis

################ ETL process ################

# Output directories

# Directory for storing downloaded ZIP files from Federal Revenue
DOWNLOAD_PATH=data/DOWNLOADED_FILES

# Directory for storing extracted CSV files from ZIP archives
EXTRACT_PATH=data/EXTRACTED_FILES

# Directory for storing converted Parquet files (optimized format)
CONVERT_PATH=data/CONVERTED_FILES

# URLs for Brazilian Federal Revenue CNPJ data

# Base URL for Brazilian Federal Revenue CNPJ data files
URL_RF_BASE=https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj

# URL for CNPJ data layout documentation (metadata PDF)
URL_RF_LAYOUT=https://www.gov.br/receitafederal/dados/cnpj-metadados.pdf

# Core ETL settings

# Timezone for processing timestamps
ETL_TIMEZONE=America/Sao_Paulo

# Delimiter for Brazilian Federal Revenue CSV files
ETL_FILE_DELIMITER=;

# Enable parallel processing across multiple cores
ETL_IS_PARALLEL=true

# Delete temporary files after successful processing
ETL_DELETE_FILES=true

# Internal concurrency level for async operations
ETL_INTERNAL_CONCURRENCY=3

# Enable comprehensive audit trail and batch tracking
ETL_MANIFEST_TRACKING=false

# Async connection pool configuration
ETL_ASYNC_POOL_MIN_SIZE=1
ETL_ASYNC_POOL_MAX_SIZE=10

# Conversion stage configuration (CSV â†’ Parquet)

# Rows per conversion batch (memory-based)
ETL_CONVERSION_CHUNK_SIZE=50000

# Maximum memory usage in MB for conversion operations
ETL_CONVERSION_MEMORY_LIMIT_MB=1024

# Number of conversion workers
ETL_CONVERSION_WORKERS=2

# Parquet compression algorithm (snappy, gzip, lz4)
ETL_CONVERSION_COMPRESSION=snappy

# Number of rows per Parquet row group for optimal querying
ETL_CONVERSION_ROW_GROUP_SIZE=100000

# Memory pressure ratio threshold for triggering cleanup (0.0-1.0)
ETL_CONVERSION_CLEANUP_THRESHOLD_RATIO=0.8

# Memory buffer in MB to maintain above baseline for system stability
ETL_CONVERSION_BASELINE_BUFFER_MB=256

# Number of files to process before flushing to disk
ETL_CONVERSION_FLUSH_THRESHOLD=10

# Automatically fallback to smaller batches on memory issues
ETL_CONVERSION_AUTO_FALLBACK=true

# Estimation factor for memory usage per row (bytes)
ETL_CONVERSION_ROW_ESTIMATION_FACTOR=8000

# Loading stage configuration (Database insertion)

# Database insertion batch size
ETL_LOADING_BATCH_SIZE=1000

# Sub-batch size for parallel processing within chunks
ETL_LOADING_SUB_BATCH_SIZE=500

# Number of loading workers
ETL_LOADING_WORKERS=3

# Maximum retry attempts for failed loading operations
ETL_LOADING_MAX_RETRIES=3

# Timeout for loading operations in seconds
ETL_LOADING_TIMEOUT_SECONDS=300

# Use COPY command for faster PostgreSQL inserts
ETL_LOADING_USE_COPY=true

# Enable parallel processing within batches
ETL_LOADING_ENABLE_INTERNAL_PARALLELISM=true

# Loading batch size limits
ETL_LOADING_MAX_BATCH_SIZE=500000
ETL_LOADING_MIN_BATCH_SIZE=10000
ETL_LOADING_BATCH_SIZE_MB=100

# Download stage configuration

# Number of download workers
ETL_DOWNLOAD_WORKERS=4

# Chunk size in MB for downloading large files
ETL_DOWNLOAD_CHUNK_SIZE_MB=50

# Verify file integrity using checksums after download
ETL_DOWNLOAD_VERIFY_CHECKSUMS=true

# File size threshold in MB for checksum verification
ETL_DOWNLOAD_CHECKSUM_THRESHOLD_MB=1000

# Timeout for download operations in seconds
ETL_DOWNLOAD_TIMEOUT_SECONDS=300

# Maximum retry attempts for failed downloads
ETL_DOWNLOAD_MAX_RETRIES=3

# Development mode settings

# File size limit in MB for development mode
ETL_DEV_FILE_SIZE_LIMIT_MB=70

# Maximum number of files to process per table in development
ETL_DEV_MAX_FILES_PER_TABLE=5

# Maximum number of files to process per blob/batch in development mode
ETL_DEV_MAX_FILES_PER_BLOB=3

# Percentage of rows to process in development mode (0.1 = 10%)
ETL_DEV_ROW_LIMIT_PERCENT=0.1

# Batch tracking configuration

# Number of operations before triggering batch status update
BATCH_UPDATE_THRESHOLD=100

# Interval in seconds between batch status updates
BATCH_UPDATE_INTERVAL=30

# Enable bulk database operations for better performance
ENABLE_BULK_UPDATES=true

# Enable temporal context tracking for time-based analysis
ENABLE_TEMPORAL_CONTEXT=true

# Default batch size for batch tracking operations
DEFAULT_BATCH_SIZE=20000

# Number of days to retain batch tracking data
BATCH_RETENTION_DAYS=30

# Enable real-time batch monitoring and alerts
ENABLE_BATCH_MONITORING=true
