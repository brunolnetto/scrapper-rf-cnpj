# Possible values: development, production
# Controls overall system behavior and enables development-specific optimizations
ENVIRONMENT=development

################ Main database information
# PostgreSQL host for main CNPJ data storage
POSTGRES_HOST=localhost

# PostgreSQL port for main database
POSTGRES_PORT=5432

# PostgreSQL username for main database
POSTGRES_USER=postgres

# PostgreSQL password for main database
POSTGRES_PASSWORD=postgres

# Main database name for storing processed CNPJ data
POSTGRES_DBNAME=dadosrfb

# Maintenance database for administrative operations
POSTGRES_MAINTENANCE_DB=postgres

################ Audit database information
# PostgreSQL host for audit/analysis database
AUDIT_DB_HOST=localhost

# PostgreSQL port for audit database
AUDIT_DB_PORT=5432

# PostgreSQL username for audit database
AUDIT_DB_USER=postgres

# PostgreSQL password for audit database
AUDIT_DB_PASSWORD=postgres

# Audit database name for storing processing metadata and logs
AUDIT_DB_NAME=dadosrfb_analysis

################ ETL process ################

# Output folders

# Directory for storing downloaded ZIP files from Federal Revenue
DOWNLOAD_PATH='DOWNLOADED_FILES'

# Directory for storing extracted CSV files from ZIP archives
EXTRACT_PATH='EXTRACTED_FILES'

# Directory for storing converted Parquet files (optimized format)
CONVERT_PATH='CONVERTED_FILES'

# URLs for Brazilian Federal Revenue CNPJ data

# Base URL for Brazilian Federal Revenue CNPJ data files
URL_RF_BASE="https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj"

# URL for CNPJ data layout documentation (metadata PDF)
URL_RF_LAYOUT="https://www.gov.br/receitafederal/dados/cnpj-metadados.pdf"

# Core settings

# Timezone for processing timestamps
ETL_TIMEZONE=America/Sao_Paulo

# Default chunk size for batch processing
ETL_CHUNK_SIZE=50000

# Sub-batch size for parallel processing within chunks
ETL_SUB_BATCH_SIZE=25000

# Maximum retry attempts for failed operations
ETL_MAX_RETRIES=3

# Number of worker processes for parallel operations
ETL_WORKERS=4

# Enable parallel processing across multiple cores
ETL_IS_PARALLEL=true

# Delete temporary files after successful processing
ETL_DELETE_FILES=true

# Delimiter for Brazilian Federal Revenue CSV files
ETL_FILE_DELIMITER=;

# Internal concurrency level for async operations
ETL_INTERNAL_CONCURRENCY=3

# Enable comprehensive audit trail and batch tracking
ETL_MANIFEST_TRACKING=false

# Minimum size of async connection pool
ETL_ASYNC_POOL_MIN_SIZE=1

# Maximum size of async connection pool
ETL_ASYNC_POOL_MAX_SIZE=10
ETL_CHECKSUM_THRESHOLD_MB=1000
ETL_TIMEOUT_SECONDS=300

# Download stage configuration

# Chunk size in MB for downloading large files
ETL_DOWNLOAD_CHUNK_SIZE_MB=50

# Verify file integrity using checksums after download
ETL_CHECKSUM_VERIFICATION=true

# Conversion configuration
# Maximum memory usage in MB for conversion operations
ETL_MAX_MEMORY_MB=1024

# Parquet compression algorithm (snappy, gzip, lz4)
ETL_COMPRESSION=snappy

# Number of rows per Parquet row group for optimal querying
ETL_ROW_GROUP_SIZE=100000

# Number of files to process before flushing to disk
ETL_CONVERSION_FLUSH_THRESHOLD=10

# Automatically fallback to smaller batches on memory issues
ETL_CONVERSION_AUTO_FALLBACK=true

# Estimation factor for memory usage per row (bytes)
ETL_CONVERSION_ROW_ESTIMATION_FACTOR=8000

# Loading configuration
<<<<<<< HEAD
# Maximum batch size for database operations
ETL_MAX_BATCH_SIZE=500000

# Minimum batch size for database operations
ETL_MIN_BATCH_SIZE=10000

# Target batch size in MB for memory management
=======
ETL_MAX_BATCH_SIZE=500_000
ETL_MIN_BATCH_SIZE=10_000
>>>>>>> 715a73e (config() Review environment variables)
ETL_BATCH_SIZE_MB=100

# Use COPY command for faster PostgreSQL inserts
ETL_USE_COPY=true

# Enable parallel processing within batches
ETL_ENABLE_INTERNAL_PARALLELISM=true

# Development mode settings
<<<<<<< HEAD
# File size limit in MB for development filtering (70MB = ~70M rows)
ETL_DEV_FILE_SIZE_LIMIT_MB=70

# Maximum number of files to process per table in development mode
=======
ETL_DEV_FILE_SIZE_LIMIT_MB=70
>>>>>>> 715a73e (config() Review environment variables)
ETL_DEV_MAX_FILES_PER_TABLE=5

# Maximum number of files to process per blob/batch in development mode
ETL_DEV_MAX_FILES_PER_BLOB=3
<<<<<<< HEAD

# Maximum blob size in MB for development mode processing
ETL_DEV_MAX_BLOB_SIZE_MB=500

# Percentage of rows to sample during loading (0.1 = 10% of rows)
=======
>>>>>>> 715a73e (config() Review environment variables)
ETL_DEV_ROW_LIMIT_PERCENT=0.1

# Batch tracking configuration
# Number of operations before triggering batch status update
BATCH_UPDATE_THRESHOLD=100

# Interval in seconds between batch status updates
BATCH_UPDATE_INTERVAL=30

# Enable bulk database operations for better performance
ENABLE_BULK_UPDATES=true

# Enable temporal context tracking for time-based analysis
ENABLE_TEMPORAL_CONTEXT=true

# Default batch size for batch tracking operations
DEFAULT_BATCH_SIZE=20000

# Number of days to retain batch tracking data
BATCH_RETENTION_DAYS=30

# Enable real-time batch monitoring and alerts
ENABLE_BATCH_MONITORING=true

# Row-driven batching configuration
# Number of rows per batch for large file processing (triggers row-driven batching)
ETL_ROW_BATCH_SIZE=10000

# Number of rows per subbatch within each batch (granular processing units)
ETL_ROW_SUBBATCH_SIZE=1000
