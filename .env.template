# Possible values: development, production
# Controls overall system behavior and enables development-specific optimizations
ENVIRONMENT=development

################ Database Configuration ################

# Main PostgreSQL database for CNPJ data storage
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DBNAME=dadosrfb
POSTGRES_MAINTENANCE_DB=postgres

# Audit database for processing metadata and logs
AUDIT_DB_HOST=localhost
AUDIT_DB_PORT=5432
AUDIT_DB_USER=postgres
AUDIT_DB_PASSWORD=postgres
AUDIT_DB_NAME=dadosrfb_analysis

################ File System Paths ################

# Directory for storing downloaded ZIP files from Federal Revenue
DOWNLOAD_PATH=data/DOWNLOADED_FILES

# Directory for storing extracted CSV files from ZIP archives
EXTRACTION_PATH=data/EXTRACTED_FILES

# Directory for storing converted Parquet files (optimized format)
CONVERSION_PATH=data/CONVERTED_FILES

################ ETL Core Settings ################

# Timezone for processing timestamps
ETL_TIMEZONE=America/Sao_Paulo

# Delimiter for Brazilian Federal Revenue CSV files
ETL_DELIMITER=;

# Encoding for dataframe characters interpretation
ETL_ENCODING=latin-1

# Enable parallel processing across multiple cores
ETL_IS_PARALLEL=true

# Delete temporary files after successful processing
ETL_DELETE_FILES=true

# Internal concurrency level for async operations
ETL_INTERNAL_CONCURRENCY=3

# Enable comprehensive audit trail and batch tracking
ETL_MANIFEST_TRACKING=false

################ Performance Settings ################

# Consolidated timeout for all operations (seconds)
ETL_TIMEOUT_SECONDS=300

# Consolidated retry attempts for failed operations
ETL_MAX_RETRIES=3

# Consolidated connection pool size
ETL_POOL_SIZE=10

################ Conversion Stage (CSV â†’ Parquet) ################

# Rows per conversion batch (memory-based)
ETL_CONVERSION_CHUNK_SIZE=50000

# Maximum memory usage in MB for conversion operations
ETL_CONVERSION_MEMORY_LIMIT_MB=1024

# Number of conversion workers
ETL_CONVERSION_WORKERS=2

# Parquet compression algorithm (snappy, gzip, lz4)
ETL_CONVERSION_COMPRESSION=snappy

################ Loading Stage (Database insertion) ################

# Database insertion batch size
ETL_LOADING_BATCH_SIZE=50000

# Number of loading workers
ETL_LOADING_WORKERS=4

################ Download Stage ################

# Number of download workers
ETL_DOWNLOAD_WORKERS=4

# Chunk size in MB for downloading large files
ETL_DOWNLOAD_CHUNK_SIZE_MB=50

# Verify file integrity using checksums after download
ETL_DOWNLOAD_VERIFY_CHECKSUMS=true

# File size threshold in MB for checksum verification
ETL_DOWNLOAD_CHECKSUM_THRESHOLD_MB=1000

################ Development Mode ################

# Enable development mode (limits file processing for faster testing)
ETL_DEV_ENABLED=true

# File size limit in MB for development mode
ETL_DEV_FILE_SIZE_LIMIT_MB=70

# Maximum number of files to process per table in development
ETL_DEV_MAX_FILES_PER_TABLE=5

# Maximum number of files to process per blob/batch in development mode
ETL_DEV_MAX_FILES_PER_BLOB=3

# Percentage of rows to process in development mode (0.1 = 10%)
ETL_DEV_ROW_LIMIT_PERCENT=0.1

################ External URLs ################

# Base URL for Brazilian Federal Revenue CNPJ data files
URL_RF_BASE=https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj

# URL for CNPJ data layout documentation (metadata PDF)
URL_RF_LAYOUT=https://www.gov.br/receitafederal/dados/cnpj-metadados.pdf

